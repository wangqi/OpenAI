//
//  ChatQuery.swift
//  
//
//  Created by Sergii Kryvoblotskyi on 02/04/2023.
//

import Foundation

public struct Chat: Codable, Equatable {
    public let role: Role
    /// The contents of the message. `content` is required for all messages except assistant messages with function calls.
    public let content: String?
    /// The name of the author of this message. `name` is required if role is `function`, and it should be the name of the function whose response is in the `content`. May contain a-z, A-Z, 0-9, and underscores, with a maximum length of 64 characters.
    public let name: String?
    public let functionCall: ChatFunctionCall?
    
    public enum Role: String, Codable, Equatable {
        case system
        case assistant
        case user
        case function
    }
    
    enum CodingKeys: String, CodingKey {
        case role
        case content
        case name
        case functionCall = "function_call"
    }
    
    public init(role: Role, content: String? = nil, name: String? = nil, functionCall: ChatFunctionCall? = nil) {
        self.role = role
        self.content = content
        self.name = name
        self.functionCall = functionCall
    }
}

public struct ChatFunctionCall: Codable, Equatable {
    /// The name of the function to call.
    public let name: String?
    /// The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
    public let arguments: String?
}

public struct ChatFunctionDeclaration: Codable, Equatable {
    /// The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
    public let name: String
    
    /// The description of what the function does.
    public let description: String
    
    /// The parameters the functions accepts, described as a JSON Schema object.
    ///
    /// See the [guide](/docs/guides/gpt/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format.
    ///
    /// In Swift there is no straightforward wat to support arbitrary JSON objects. Dictionary<String, Any> is not convenient as it requires explicit type annotations for all nested objects. Here we use structured approach
    ///
    /// ChatFunctionDeclaration(
    ///     name: "get_current_weather",
    ///     description: "Get the current weather in a given location",
    ///     parameters: [
    ///         "type": .string("object"),
    ///         "properties": .dictionary([
    ///             "location": .dictionary([
    ///                 "type": .string("string"),
    ///                 "description": .string("The city and state, e.g. San Francisco, CA")
    ///             ]),
    ///             "unit": .dictionary([
    ///                 "type": .string("string"),
    ///                 "enum": .array([.string("celsius"), .string("fahrenheit")])
    ///             ])
    ///         ]),
    ///         "required": .array([.string("location")])
    ///     ]
    /// )
    /// This allows Xcode auto-complete to work.
    public let parameters: [String: JSON]
  
    public init(name: String, description: String, parameters: [String : JSON]) {
      self.name = name
      self.description = description
      self.parameters = parameters
    }

    public enum JSON: Codable, Equatable {
        case string(String)
        case int(Int)
        case bool(Bool)
        case array([JSON])
        case dictionary([String: JSON])

        public init(from decoder: Decoder) throws {
            let container = try decoder.singleValueContainer()
            if let x = try? container.decode(String.self) { self = .string(x); return }
            if let x = try? container.decode(Int.self) { self = .int(x); return }
            if let x = try? container.decode(Bool.self) { self = .bool(x); return }
            if let x = try? container.decode([JSON].self) { self = .array(x); return }
            if let x = try? container.decode([String: JSON].self) { self = .dictionary(x); return }
            throw DecodingError.typeMismatch(JSON.self, DecodingError.Context(codingPath: decoder.codingPath, debugDescription: "Unexpected type for JSON"))
        }

        public func encode(to encoder: Encoder) throws {
            var container = encoder.singleValueContainer()
            switch self {
            case .string(let x): try container.encode(x)
            case .int(let x): try container.encode(x)
            case .bool(let x): try container.encode(x)
            case .array(let x): try container.encode(x)
            case .dictionary(let x): try container.encode(x)
            }
        }
    }
}

public struct ChatQueryFunctionCall: Codable, Equatable {
    /// The name of the function to call.
    public let name: String?
    /// The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
    public let arguments: String?
}

public struct ChatQuery: Equatable, Codable, Streamable {
    /// ID of the model to use. Currently, only gpt-3.5-turbo and gpt-3.5-turbo-0301 are supported.
    public let model: Model
    /// The messages to generate chat completions for
    public let messages: [Chat]
    /// A list of functions the model may generate JSON inputs for.
    public let functions: [ChatFunctionDeclaration]?
    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and  We generally recommend altering this or top_p but not both.
    public let temperature: Double?
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    public let topP: Double?
    /// How many chat completion choices to generate for each input message.
    public let n: Int?
    /// Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
    public let stop: [String]?
    /// The maximum number of tokens to generate in the completion.
    public let maxTokens: Int?
    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
    public let presencePenalty: Double?
    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
    public let frequencyPenalty: Double?
    /// Modify the likelihood of specified tokens appearing in the completion.
    public let logitBias: [String:Int]?
    /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
    public let user: String?
    
    var stream: Bool = false
    
    enum CodingKeys: String, CodingKey {
        case model
        case messages
        case functions
        case temperature
        case topP = "top_p"
        case n
        case stream
        case stop
        case maxTokens = "max_tokens"
        case presencePenalty = "presence_penalty"
        case frequencyPenalty = "frequency_penalty"
        case logitBias = "logit_bias"
        case user
    }
    
  public init(model: Model, messages: [Chat], functions: [ChatFunctionDeclaration]? = nil, temperature: Double? = nil, topP: Double? = nil, n: Int? = nil, stop: [String]? = nil, maxTokens: Int? = nil, presencePenalty: Double? = nil, frequencyPenalty: Double? = nil, logitBias: [String : Int]? = nil, user: String? = nil, stream: Bool = false) {
        self.model = model
        self.messages = messages
        self.functions = functions
        self.temperature = temperature
        self.topP = topP
        self.n = n
        self.stop = stop
        self.maxTokens = maxTokens
        self.presencePenalty = presencePenalty
        self.frequencyPenalty = frequencyPenalty
        self.logitBias = logitBias
        self.user = user
        self.stream = stream
    }
}
